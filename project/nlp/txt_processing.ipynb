{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk spacy huspacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import huspacy\n",
    "import spacy\n",
    "huspacy.download()\n",
    "nlp = spacy.load('hu_core_news_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"lép\", \"ablak\", \"eszik\", \"iszik\", \"tábla\", \"toll\", \"toll\"]\n",
    "pprint.pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = {}\n",
    "for word in data:\n",
    "    if word in word_freq:\n",
    "        word_freq[word] += 1\n",
    "    else:\n",
    "        word_freq[word] = 1\n",
    "\n",
    "pprint.pprint(word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = []\n",
    "for word in word_freq:\n",
    "    if word_freq[word] > 1:\n",
    "        common_words.append(word)\n",
    "        \n",
    "pprint.pprint(common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization - NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'világ', '!', 'Hogy', 'vagy', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Hello, világ! Hogy vagy?\"\n",
    "words = word_tokenize(text)\n",
    "pprint.pprint(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word tokenization - spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'világ', '!', 'Hogy', 'vagy', '?']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "text = \"Hello, világ! Hogy vagy?\"\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "pprint.pprint(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentene tokenization - NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, világ!', 'Hogy vagy?', 'Én jól vagyok.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Hello, világ! Hogy vagy? Én jól vagyok.\"\n",
    "sents = sent_tokenize(text)\n",
    "pprint.pprint(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence tokenization - spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hello, világ!, Hogy vagy?, Én jól vagyok., asd.]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "text = \"Hello, világ! Hogy vagy? Én jól vagyok.\"\n",
    "sents = list(doc.sents)\n",
    "\n",
    "pprint.pprint(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "pprint.pprint(remove_punctuation(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stopwords - NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'világ', '!', '?', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"hungarian\"))\n",
    "words = word_tokenize(text)\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "pprint.pprint(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stopwords - spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'világ', '!', '?', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, világ! Hogy vagy? Én jól vagyok.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "filtered_words = [token.text for token in doc if not token.is_stop]\n",
    "pprint.pprint(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming - Lemmatization - NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hell', ',', 'világ', '!', 'hogy', 'vagy', '?', 'én', 'jól', 'vagy', '.']\n",
      "['Hello', ',', 'világ', '!', 'Hogy', 'vagy', '?', 'Én', 'jól', 'vagyok', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "stemmer = SnowballStemmer(\"hungarian\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = word_tokenize(text)\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "pprint.pprint(stemmed_words)\n",
    "pprint.pprint(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming - Lemmatization - spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hell', ',', 'világ', '!', 'hogy', 'vagy', '?', 'én', 'jól', 'vagy', '.']\n",
      "['Hello', ',', 'világ', '!', 'hogy', 'vagy', '?', 'én', 'jól', 'van', '.']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "stemmed_words = [stemmer.stem(token.text) for token in doc]\n",
    "lemmatized_words = [token.lemma_ for token in doc]\n",
    "\n",
    "pprint.pprint(stemmed_words)\n",
    "pprint.pprint(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_text = \"Nyolc 8\"\n",
    "pprint.pprint(re.sub(r\"\\d+\", \"\", another_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(re.sub(r\"[^a-zA-Z0-9áeéiíóöőúüűÁEÉIÍÓÖŐÚÜŰ]\", \"\", text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(re.sub(r\"http\\S+\", \"\", \"https://www.google.com/search?q=python programming\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(re.sub(r\"<.*?>\", \"\", \"<p>hello</p>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing whitespace (leading and trailing whitespace removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_text = \"    Ez egy     szöveg.    \"\n",
    "pprint.pprint(another_text.strip())\n",
    "pprint.pprint(re.sub(r\"\\s+\", \" \", another_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "import nltk\n",
    "\n",
    "# Creating a vocabulary\n",
    "vocab = set()\n",
    "\n",
    "# Creating the BoW model\n",
    "bow_model = []\n",
    "\n",
    "text_data = [\n",
    "    \"John likes to watch movies. Mary likes movies too.\",\n",
    "    \"John also likes to watch football games.\",\n",
    "]\n",
    "\n",
    "for text in text_data:\n",
    "    # Creating a dictionary for the word frequency table\n",
    "    word_freq = dict()\n",
    "    \n",
    "    # Tokenizing the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Converting the tokens into lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Updating the vocabulary\n",
    "    vocab.update(tokens)\n",
    "    \n",
    "    # Updating the word frequency table\n",
    "    for token in tokens:\n",
    "        if token in word_freq:\n",
    "            word_freq[token] += 1\n",
    "        else:\n",
    "            word_freq[token] = 1\n",
    "            \n",
    "    # Appending the word frequency table to the BoW model\n",
    "    bow_model.append(word_freq)\n",
    "    \n",
    "# Printing the vocabulary\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "# Printing the BoW model in a more readable format\n",
    "for i, item in enumerate(bow_model):\n",
    "    print(\"Text\", i + 1, \":\", item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-grams\n",
    "- Speech Recognition\n",
    "- Machine Translation\n",
    "- Predictive Text Input\n",
    "- Named Entity Recognition (NER)\n",
    "- Search Engine Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "pprint.pprint(\"N-grams\")\n",
    "n_grams = ngrams(words, 4)\n",
    "for grams in n_grams:\n",
    "    pprint.pprint(grams)\n",
    "      \n",
    "bigrams = ngrams(words, 2)\n",
    "for grams in bigrams:\n",
    "    pprint.pprint(grams)\n",
    "\n",
    "trigrams = ngrams(words, 3)\n",
    "for grams in trigrams:\n",
    "    pprint.pprint(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-gram predict next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted next word after \"franciaország\" is \"összehozta\", Probability: 1.00\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import islice\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Process the text\n",
    "text = \"CSAK A KÜZDELEM: HOLLANDIA ÉS FRANCIAORSZÁG ÖSSZEHOZTA AZ IDEI LABDARÚGÓ EB ELSŐ GÓL NÉLKÜLI EREDMÉNYÉT\".lower()\n",
    "doc = nlp(text)\n",
    "\n",
    "# Tokenize and preprocess the text\n",
    "tokens = [token.text for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "bigrams = ngrams(tokens, 2)\n",
    "\n",
    "# Count bigrams and unigrams\n",
    "bigram_counts = Counter(bigrams)\n",
    "unigram_counts = Counter(tokens)\n",
    "\n",
    "# Calculate bigram probabilities: P(w2|w1) = count(w1, w2) / count(w1)\n",
    "bigram_prob = defaultdict(lambda: defaultdict(float))\n",
    "for (w1, w2), count in bigram_counts.items():\n",
    "    bigram_prob[w1][w2] = count / unigram_counts[w1]\n",
    "\n",
    "# Function to predict next word\n",
    "def predict_next_word(word, bigram_prob):\n",
    "    if word in bigram_prob:\n",
    "        next_word = max(bigram_prob[word], key=bigram_prob[word].get, default=None)\n",
    "    else:\n",
    "        next_word = None\n",
    "    return next_word\n",
    "\n",
    "# Predict next word\n",
    "previous_word = 'franciaország'\n",
    "predicted_word = predict_next_word(previous_word, bigram_prob)\n",
    "probability = bigram_prob[previous_word][predicted_word] if predicted_word else 0\n",
    "print(f'The predicted next word after \"{previous_word}\" is \"{predicted_word}\", Probability: {probability:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-grams predict next sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted next sentence after \"ma szombat van.\" is \"az időjárás szép.\", Probability: 1.0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import islice\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Sample text data\n",
    "text = \"\"\"\n",
    "Az ég kék.\n",
    "Ma szombat van.\n",
    "Az időjárás szép.\n",
    "Esik az eső.\n",
    "\"\"\"\n",
    "# Preprocess the text into sentences\n",
    "doc = nlp(text.lower())\n",
    "sentences = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "sentence_bigrams = ngrams(sentences, 2)\n",
    "\n",
    "# Count sentence bigrams and unigrams\n",
    "bigram_counts = Counter(sentence_bigrams)\n",
    "unigram_counts = Counter(sentences)\n",
    "\n",
    "# Calculate bigram probabilities: P(s2|s1) = count(s1, s2) / count(s1)\n",
    "bigram_prob = defaultdict(lambda: defaultdict(float))\n",
    "for (s1, s2), count in bigram_counts.items():\n",
    "    bigram_prob[s1][s2] = count / unigram_counts[s1]\n",
    "\n",
    "# Function to predict next sentence\n",
    "def predict_next_sentence(sentence, bigram_prob):\n",
    "    if sentence in bigram_prob:\n",
    "        next_sentence = max(bigram_prob[sentence], key=bigram_prob[sentence].get, default=None)\n",
    "    else:\n",
    "        next_sentence = None\n",
    "    return next_sentence\n",
    "\n",
    "# Predict next sentence\n",
    "previous_sentence = 'ma szombat van.'\n",
    "predicted_sentence = predict_next_sentence(previous_sentence, bigram_prob)\n",
    "probability = bigram_prob[previous_sentence][predicted_sentence] if predicted_sentence else 0\n",
    "print(f'The predicted next sentence after \"{previous_sentence}\" is \"{predicted_sentence}\", Probability: {probability}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part of Speech Tagging (POS)\n",
    "|**Part of Speech**|**Tag**|\n",
    "|----|-|\n",
    "|Noun|n|\n",
    "|Verb|v|\n",
    "|Adjective|a|\n",
    "|Adverb|r|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amikor ADV ADV\n",
      "a DET DET\n",
      "nyáj NOUN NOUN\n",
      "elbóklászik VERB VERB\n",
      ", PUNCT PUNCT\n",
      "a DET DET\n",
      "pásztornak NOUN NOUN\n",
      "meg PART PART\n",
      "kell VERB VERB\n",
      "ölnie VERB VERB\n",
      "a DET DET\n",
      "szakadár NOUN ADJ\n",
      "vezér NOUN NOUN\n",
      "ürüt NOUN NOUN\n"
     ]
    }
   ],
   "source": [
    "text = \"Amikor a nyáj elbóklászik, a pásztornak meg kell ölnie a szakadár vezér ürüt\"\n",
    "\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition (NER)\n",
    "- Process to locate and classify named entities in text into predefined categories such as names of persons, organizations, locations, expressions of times, quantities, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/0xbalazstoth/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/0xbalazstoth/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lance Stroll PER\n",
      "Hamiltonra LOC\n",
      "Leclerc PER\n",
      "Norris PER\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Lance Stroll\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " ezúttal sem kimagasló teljesítményével hívta fel magára figyelmet, hanem azzal, hogy egyenesen ráhúzta a kormányt a pálya külső ívén érkező \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Hamiltonra\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". A két autó kissé össze is ért, az esetet az edzést követően megvizsgálják a sportfelügyelők. Hasonló manőver játszódott le egyébként \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Leclerc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " és \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Norris\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " között is a tréning legvégén.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"hu_core_news_lg\")\n",
    "\n",
    "text = \"Lance Stroll ezúttal sem kimagasló teljesítményével hívta fel magára figyelmet, hanem azzal, hogy egyenesen ráhúzta a kormányt a pálya külső ívén érkező Hamiltonra. A két autó kissé össze is ért, az esetet az edzést követően megvizsgálják a sportfelügyelők. Hasonló manőver játszódott le egyébként Leclerc és Norris között is a tréning legvégén.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "    \n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "- A TF-IDF (Term Frequency-Inverse Document Frequency) egy statisztikai mérőszám, amely a dokumentumok szövegeiben szereplő szavak fontosságát méri.\n",
    "- Két fő komponense van: TF (Term Frequency) és IDF (Inverse Document Frequency).\n",
    "\n",
    "- TF (Term Frequency)\n",
    "A TF egy adott szó gyakoriságát méri egy adott dokumentumban. Minél többször fordul elő egy szó egy dokumentumban, annál magasabb lesz a TF értéke.\n",
    "    - TF(t, d) = (Number of times term t appears in document d) / (Total number of terms in document d)\n",
    "- DF (Inverse Document Frequency)\n",
    "Az IDF egy szó fontosságát méri az egész dokumentumkorpuszban. Az IDF célja, hogy csökkentse azoknak a szavaknak a súlyát, amelyek gyakran előfordulnak a dokumentumkorpuszban (például \"az\", \"és\", \"van\"), mivel ezek a szavak kevés információval bírnak a dokumentumok tartalmát illetően.\n",
    "    - IDF(t, D) = log(Total number of documents D / Number of documents with term t)\n",
    "- TF-IDF\n",
    "A TF-IDF a TF és az IDF szorzata. Ezzel a szorzattal a TF-IDF érték magas lesz, ha egy szó gyakran fordul elő egy adott dokumentumban, de ritkán az egész dokumentumkorpuszban. Ezáltal a TF-IDF képes kiemelni a dokumentumokra jellemző szavakat, miközben figyelmen kívül hagyja a gyakori, általános szavakat.\n",
    "    - TF-IDF(t, d, D) = TF(t, d) * IDF(t, D)\n",
    "\n",
    "- Miért hasznos?\n",
    "  - Szövegbányászat, információkeresés\n",
    "  - Dokumentum klaszterezés, osztályozás\n",
    "  - Szöveganalitika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fényes       kék    láthat       nap    ragyog   ragyogó        ég\n",
      "0  0.000000  0.785288  0.000000  0.000000  0.000000  0.000000  0.619130\n",
      "1  0.000000  0.000000  0.000000  0.629228  0.777221  0.000000  0.000000\n",
      "2  0.000000  0.000000  0.000000  0.496816  0.613667  0.000000  0.613667\n",
      "3  0.464757  0.000000  0.464757  0.593297  0.000000  0.464757  0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "documents = [\n",
    "    \"Az ég kék.\",\n",
    "    \"A nap ragyog.\",\n",
    "    \"A nap az égen ragyog.\",\n",
    "    \"Láthatjuk a ragyogó napot, a fényes napot.\"\n",
    "]\n",
    "\n",
    "# Function to preprocess text using HuSpaCy\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocess the documents\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in documents]\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed documents\n",
    "tfidf_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "# Create a DataFrame to display the TF-IDF scores\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the TF-IDF scores\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- [NLTK book](https://www.nltk.org/book/)\n",
    "- [Roadmap](https://medium.com/aimonks/roadmap-to-learn-natural-language-processing-in-2023-6e3a9372b8cc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
